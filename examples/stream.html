<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>StreamTranscriber Example | Transcribe.js</title>
    <link
      rel="stylesheet"
      href="https://cdn.jsdelivr.net/npm/water.css@2/out/water.css"
    />
    <link rel="stylesheet" href="css/theme.css" />
  </head>
  <body>
    <script src="model-loader.js"></script>

    <script type="module">
      import { simd } from "./lib/simd.js";
      import { default as createModuleSimd } from "../src/shout/shout.wasm.js?v1.0.4";
      import { default as createModuleNoSimd } from "../src/shout/shout.wasm_no-simd.js?v1.0.4";
      import { StreamTranscriber } from "../src/index.js?v2.0.5";

      (async function () {
        // feature detect simd support
        // only if you need to support browsers/devices without simd support
        const createModule = (await simd())
          ? createModuleSimd
          : createModuleNoSimd;

        // Preload model
        const path = location.pathname.replace(/\/[^/]+$/, "");
        const modelFile = `${path}/models/ggml-tiny-q5_1.bin`;

        preloadModel(
          modelFile,
          (msg) => (document.querySelector(".status").innerText = msg)
        ).then((loaded) => {
          if (loaded) {
            document.querySelector(".audio").removeAttribute("disabled");
            document.querySelector(".start").removeAttribute("disabled");
            document.querySelector(".stop").removeAttribute("disabled");
          }
        });

        // callback for voice activity
        const onVoiceActivity = (active) => {
          document.querySelector(".vad").innerText = active ? "ðŸ”´" : "";
        };

        // create a new transcriber instance with callbacks
        const transcriber = new StreamTranscriber({
          createModule,

          // model, you can try larger models, but they tend to be too slow for realtime transcription
          model: modelFile,

          // on transcribe callback
          onSegment: (result) => {
            document.querySelector(".transcript").innerText =
              result?.segment?.text ?? "";
          },

          // on status change callback
          onStreamStatus: (status) => {
            document.querySelector(".status").innerText = status;
          },
        });

        // initialize the transcriber
        async function init() {
          try {
            await transcriber.init();
          } catch (error) {
            console.error("Error transcribing", error);
          }
        }

        // stream audio from a file
        let audio;
        async function streamAudio() {
          audio?.pause();
          audio = null;

          await init();
          // start waiting for audio
          await transcriber.start({ lang: "en" });

          audio = new Audio("/examples/albert.ogg");
          await audio.play();
          const stream = audio.captureStream
            ? audio.captureStream()
            : audio.mozCaptureStream();

          transcriber.transcribe(stream, { onVoiceActivity });
        }

        async function stream() {
          await init();
          // start waiting for audio
          await transcriber.start({ lang: "en" });

          // get the microphone audio stream
          navigator.mediaDevices
            .getUserMedia({ audio: true, video: false })
            .then((stream) => {
              // transcribe the audio stream
              transcriber.transcribe(stream, {
                preRecordMs: 1000, // add 1s before actual speech got detected
                maxRecordMs: 10000, // force transcribe after 10s
                minSilenceMs: 700, // min time of silence before transcribe
                onVoiceActivity,
              });
            });
        }

        // start the audio stream
        document.querySelector(".audio").addEventListener("click", () => {
          streamAudio();
        });

        // start the stream
        document.querySelector(".start").addEventListener("click", () => {
          stream();
        });

        // stop the stream
        document.querySelector(".stop").addEventListener("click", async () => {
          await transcriber.stop();
          audio?.pause();
        });
      })();
    </script>

    <main class="flow">
      <h1>Transcribe.js - StreamTranscriber (experimental)</h1>
      <p>
        <a href="https://transcribejs.dev">Transcribe.js</a> <br />
        <a href="https://github.com/transcribejs/transcribe.js"
          >Code on Github</a
        >
      </p>
      <p>This is more a proof of concept than a production ready solution.</p>
      <p>
        It <b>doesn't work in Firefox</b> because
        <a href="https://bugzilla.mozilla.org/show_bug.cgi?id=1725336"
          >AudioContext is not working</a
        >
        with a custom sample rate.
      </p>
      <p>
        Also shout wasm is
        <strike>way too slow</strike>
        <em>depends on your machine and the model size</em> to acutally
        transcribe in realtime. But apart from that it works better than I've
        expected.
      </p>
      <p>
        <button class="audio">Start Audio Stream</button>
        <button class="start">Start Microphone Stream</button>
        <button class="stop">Stop Stream</button>
      </p>
      <div style="display: flex; column-gap: 1rem; align-items: center">
        <div style="min-width: 15rem">
          <b>Voice Activity:</b>
          <span class="vad" style="font-size: 0.75em"></span>
        </div>
        <div>
          <b>Status:</b> <code class="status"><em>click start</em></code>
        </div>
      </div>
      <pre><code class="transcript" style="white-space: normal;"></code></pre>
    </main>
  </body>
</html>
